{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will create a model to predict the probability of a team winning an NCAA tournament game against another team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the prediction data created in MatchupFeatureEngineering.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weighted_Rating_x</th>\n",
       "      <th>Weighted_Rating_y</th>\n",
       "      <th>Colley_Rating_x</th>\n",
       "      <th>Teamrank_Rating_x</th>\n",
       "      <th>Teamrank10_Rating_x</th>\n",
       "      <th>Trank_OE_x</th>\n",
       "      <th>Trank_DE_x</th>\n",
       "      <th>Trank_Rating_x</th>\n",
       "      <th>EFG%_x</th>\n",
       "      <th>EFGD%_x</th>\n",
       "      <th>...</th>\n",
       "      <th>yOffxDefAstDiff</th>\n",
       "      <th>xOffyDefPoints3</th>\n",
       "      <th>yOffxDefPoints3</th>\n",
       "      <th>xOffyDefPoints2</th>\n",
       "      <th>yOffxDefPoints2</th>\n",
       "      <th>xOffyDefPoints1</th>\n",
       "      <th>yOffxDefPoints1</th>\n",
       "      <th>xOffyDefPoints</th>\n",
       "      <th>yOffxDefPoints</th>\n",
       "      <th>PointDiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.603241</td>\n",
       "      <td>9.928060</td>\n",
       "      <td>0.484032</td>\n",
       "      <td>0.446465</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>97.3</td>\n",
       "      <td>99.1</td>\n",
       "      <td>0.445334</td>\n",
       "      <td>49.8</td>\n",
       "      <td>46.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021118</td>\n",
       "      <td>0.363127</td>\n",
       "      <td>0.263351</td>\n",
       "      <td>0.319767</td>\n",
       "      <td>0.290067</td>\n",
       "      <td>0.275786</td>\n",
       "      <td>0.267921</td>\n",
       "      <td>0.958679</td>\n",
       "      <td>0.821339</td>\n",
       "      <td>0.137340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.358259</td>\n",
       "      <td>23.232983</td>\n",
       "      <td>0.921698</td>\n",
       "      <td>0.921212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>117.2</td>\n",
       "      <td>88.8</td>\n",
       "      <td>0.978279</td>\n",
       "      <td>54.1</td>\n",
       "      <td>47.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152258</td>\n",
       "      <td>0.403022</td>\n",
       "      <td>0.374303</td>\n",
       "      <td>0.328184</td>\n",
       "      <td>0.314259</td>\n",
       "      <td>0.263955</td>\n",
       "      <td>0.230452</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.919015</td>\n",
       "      <td>0.076146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.358259</td>\n",
       "      <td>36.520674</td>\n",
       "      <td>0.921698</td>\n",
       "      <td>0.921212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>117.2</td>\n",
       "      <td>88.8</td>\n",
       "      <td>0.978279</td>\n",
       "      <td>54.1</td>\n",
       "      <td>47.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109056</td>\n",
       "      <td>0.394202</td>\n",
       "      <td>0.307715</td>\n",
       "      <td>0.311970</td>\n",
       "      <td>0.314546</td>\n",
       "      <td>0.275416</td>\n",
       "      <td>0.234707</td>\n",
       "      <td>0.981588</td>\n",
       "      <td>0.856967</td>\n",
       "      <td>0.124620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.274341</td>\n",
       "      <td>36.520674</td>\n",
       "      <td>0.865920</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.848148</td>\n",
       "      <td>115.2</td>\n",
       "      <td>92.3</td>\n",
       "      <td>0.943463</td>\n",
       "      <td>55.0</td>\n",
       "      <td>46.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>0.387736</td>\n",
       "      <td>0.377983</td>\n",
       "      <td>0.319887</td>\n",
       "      <td>0.311065</td>\n",
       "      <td>0.302387</td>\n",
       "      <td>0.233008</td>\n",
       "      <td>1.010010</td>\n",
       "      <td>0.922057</td>\n",
       "      <td>0.087953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.312421</td>\n",
       "      <td>25.040472</td>\n",
       "      <td>0.938298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961111</td>\n",
       "      <td>121.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>56.3</td>\n",
       "      <td>44.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078373</td>\n",
       "      <td>0.347188</td>\n",
       "      <td>0.439235</td>\n",
       "      <td>0.358261</td>\n",
       "      <td>0.316658</td>\n",
       "      <td>0.257623</td>\n",
       "      <td>0.231491</td>\n",
       "      <td>0.963072</td>\n",
       "      <td>0.987385</td>\n",
       "      <td>-0.024313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Weighted_Rating_x  Weighted_Rating_y  Colley_Rating_x  Teamrank_Rating_x  \\\n",
       "0          19.603241           9.928060         0.484032           0.446465   \n",
       "1          40.358259          23.232983         0.921698           0.921212   \n",
       "2          40.358259          36.520674         0.921698           0.921212   \n",
       "3          37.274341          36.520674         0.865920           0.822222   \n",
       "4          42.312421          25.040472         0.938298           1.000000   \n",
       "\n",
       "   Teamrank10_Rating_x  Trank_OE_x  Trank_DE_x  Trank_Rating_x  EFG%_x  \\\n",
       "0             0.444444        97.3        99.1        0.445334    49.8   \n",
       "1             1.000000       117.2        88.8        0.978279    54.1   \n",
       "2             1.000000       117.2        88.8        0.978279    54.1   \n",
       "3             0.848148       115.2        92.3        0.943463    55.0   \n",
       "4             0.961111       121.0        85.6        1.000000    56.3   \n",
       "\n",
       "   EFGD%_x    ...      yOffxDefAstDiff  xOffyDefPoints3  yOffxDefPoints3  \\\n",
       "0     46.5    ...             0.021118         0.363127         0.263351   \n",
       "1     47.6    ...             0.152258         0.403022         0.374303   \n",
       "2     47.6    ...             0.109056         0.394202         0.307715   \n",
       "3     46.8    ...             0.018604         0.387736         0.377983   \n",
       "4     44.8    ...             0.078373         0.347188         0.439235   \n",
       "\n",
       "   xOffyDefPoints2  yOffxDefPoints2  xOffyDefPoints1  yOffxDefPoints1  \\\n",
       "0         0.319767         0.290067         0.275786         0.267921   \n",
       "1         0.328184         0.314259         0.263955         0.230452   \n",
       "2         0.311970         0.314546         0.275416         0.234707   \n",
       "3         0.319887         0.311065         0.302387         0.233008   \n",
       "4         0.358261         0.316658         0.257623         0.231491   \n",
       "\n",
       "   xOffyDefPoints  yOffxDefPoints  PointDiff  \n",
       "0        0.958679        0.821339   0.137340  \n",
       "1        0.995161        0.919015   0.076146  \n",
       "2        0.981588        0.856967   0.124620  \n",
       "3        1.010010        0.922057   0.087953  \n",
       "4        0.963072        0.987385  -0.024313  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchups = pd.read_csv('mydata/matchups.csv')\n",
    "matchups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = matchups.Upset\n",
    "X = matchups.drop(columns = ['Upset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "The first model we will be using is Logistic Regression. Logistic regression is good at predicting probabilities and L1 regression is helpful for dimension reduction to find the most important features. We will fit two models, one with L1 regularization and the other with L2 regularization. We will use cross validation to find the optimal regularization coefficients that minimize log loss to make sure we are not overfitting. Then we will test the models on an unseen testing set and assess their log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [.0003, .001, .003, .01, .03, .1, .3, 1, 3, 10]\n",
    "log1 = LogisticRegressionCV(cv = 9, random_state = 0, solver = 'liblinear', penalty = 'l1', Cs = c, scoring = 'neg_log_loss').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2 = LogisticRegressionCV(cv = 9, random_state = 0, solver = 'liblinear', penalty = 'l2', Cs = c, scoring = 'neg_log_loss').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5524384227555681\n",
      "-0.5523004897146524\n"
     ]
    }
   ],
   "source": [
    "# Prints the average cv log loss for the model chosen in cross validation\n",
    "avg_log_loss1 = []\n",
    "avg_log_loss2 = []\n",
    "for i in range(len(log1.scores_[1][0])):\n",
    "    loss1 = 0\n",
    "    loss2 = 0\n",
    "    for j in range(len(log1.scores_[1])):\n",
    "        loss1 = loss1 + log1.scores_[1][j][i]\n",
    "        loss2 = loss2 + log2.scores_[1][j][i]\n",
    "    avg_log_loss1.append(loss1 / len(log1.scores_[1]))\n",
    "    avg_log_loss2.append(loss2 / len(log2.scores_[1]))\n",
    "print(max(avg_log_loss1))  # best avg cv log loss for logistic regression model with L1 penalty\n",
    "print(max(avg_log_loss2))  # best avg cv log loss for logistic regression model with L2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584843643531568\n",
      "0.5863985166050165\n",
      "0.5854963134931546\n"
     ]
    }
   ],
   "source": [
    "predict1 = log1.predict_proba(X_test)\n",
    "predict2 = log2.predict_proba(X_test)\n",
    "predict3 = (predict1 + predict2) / 2\n",
    "actual_log_loss1 = log_loss(y_test, predict1)\n",
    "actual_log_loss2 = log_loss(y_test, predict2)\n",
    "actual_log_loss3 = log_loss(y_test, predict3)\n",
    "print(actual_log_loss1)\n",
    "print(actual_log_loss2)\n",
    "print(actual_log_loss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log1.C_  # The regularization coefficient found in cross validation for L1 penalty logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.003])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log2.C_  # The regularization coefficient found in cross validation for L2 penalty logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "The second model we will be using is Random Forest. Random Forest is robust to nonlinear decision boundries compared to logistic regression so it might perform better. We will use cross validation to find the optimal hyperparamters that minimize log loss to make sure we are not overfitting. Then we will test the model on an unseen testing set and assess its log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for random forest\n",
    "rf_params = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(50, 1000, 20)],\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    'max_depth': [10, 20, 30, 40, None],\n",
    "    'min_samples_split': [int(x) for x in np.linspace(2, 10, 9)],\n",
    "    'max_features': [4, 5, 'sqrt', 'log2', None],\n",
    "    'random_state': [0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "random_search = RandomizedSearchCV(rf, param_distributions = rf_params, cv = 9, n_iter = 100, random_state = 0, \n",
    "                                   scoring = 'neg_log_loss').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=10, max_features=4, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=8,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best hyperparamaters found in the random grid search to minimize average log loss\n",
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5510799178506031"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best average log loss achieved through the random grid search\n",
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6199459847111619"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The log loss of the random forest model on the unseen testing set\n",
    "predict = random_search.predict_proba(X_test)\n",
    "log_loss(y_test, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors\n",
    "\n",
    "The third model we will be using is K Nearest Neighbors. KNN is a simple model that is easy to explain. One problem with our dataset is that it contains 100+ features, some of which are not important. We do not want to use these unimportant features to determine similarity. Therefore, we will use our previous logistic regression model with L1 penalty for dimension reduction. The L1 model zeroes out features that are highly correlated or unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weighted_Rating_y</th>\n",
       "      <th>Trank_OE_x</th>\n",
       "      <th>Trank_DE_x</th>\n",
       "      <th>EFGD%_x</th>\n",
       "      <th>TOR_x</th>\n",
       "      <th>TORD_x</th>\n",
       "      <th>ORB_x</th>\n",
       "      <th>DRB_x</th>\n",
       "      <th>seed_x</th>\n",
       "      <th>Trank_DE_y</th>\n",
       "      <th>...</th>\n",
       "      <th>SeedDiff</th>\n",
       "      <th>WeightedRatingDiff</th>\n",
       "      <th>TrankTempoDiff</th>\n",
       "      <th>KenpomTempoAbsDiff</th>\n",
       "      <th>xOffyDefTrankAvg</th>\n",
       "      <th>yOffxDefTrankAvg</th>\n",
       "      <th>yOffxDefTODiff</th>\n",
       "      <th>xOffyDefTOAvg</th>\n",
       "      <th>xOffyDefRebDiff</th>\n",
       "      <th>yOffxDefRebDiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.928060</td>\n",
       "      <td>97.3</td>\n",
       "      <td>99.1</td>\n",
       "      <td>46.5</td>\n",
       "      <td>20.8</td>\n",
       "      <td>21.8</td>\n",
       "      <td>30.1</td>\n",
       "      <td>32.7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>105.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.675181</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2.9896</td>\n",
       "      <td>101.4</td>\n",
       "      <td>94.65</td>\n",
       "      <td>0.4</td>\n",
       "      <td>22.85</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.232983</td>\n",
       "      <td>117.2</td>\n",
       "      <td>88.8</td>\n",
       "      <td>47.6</td>\n",
       "      <td>18.2</td>\n",
       "      <td>24.9</td>\n",
       "      <td>34.3</td>\n",
       "      <td>33.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>17.125276</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.8618</td>\n",
       "      <td>111.1</td>\n",
       "      <td>97.15</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>20.30</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.520674</td>\n",
       "      <td>117.2</td>\n",
       "      <td>88.8</td>\n",
       "      <td>47.6</td>\n",
       "      <td>18.2</td>\n",
       "      <td>24.9</td>\n",
       "      <td>34.3</td>\n",
       "      <td>33.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>92.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>3.837585</td>\n",
       "      <td>7.9</td>\n",
       "      <td>7.8767</td>\n",
       "      <td>104.7</td>\n",
       "      <td>100.95</td>\n",
       "      <td>-8.6</td>\n",
       "      <td>20.75</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.520674</td>\n",
       "      <td>115.2</td>\n",
       "      <td>92.3</td>\n",
       "      <td>46.8</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>29.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>92.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.753667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1964</td>\n",
       "      <td>103.7</td>\n",
       "      <td>102.70</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>21.80</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.040472</td>\n",
       "      <td>121.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>44.8</td>\n",
       "      <td>18.7</td>\n",
       "      <td>22.9</td>\n",
       "      <td>38.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.8</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>17.271949</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0755</td>\n",
       "      <td>111.4</td>\n",
       "      <td>95.45</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>20.10</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Weighted_Rating_y  Trank_OE_x  Trank_DE_x  EFGD%_x  TOR_x  TORD_x  ORB_x  \\\n",
       "0           9.928060        97.3        99.1     46.5   20.8    21.8   30.1   \n",
       "1          23.232983       117.2        88.8     47.6   18.2    24.9   34.3   \n",
       "2          36.520674       117.2        88.8     47.6   18.2    24.9   34.3   \n",
       "3          36.520674       115.2        92.3     46.8   20.3    20.0   36.5   \n",
       "4          25.040472       121.0        85.6     44.8   18.7    22.9   38.0   \n",
       "\n",
       "   DRB_x  seed_x  Trank_DE_y       ...         SeedDiff  WeightedRatingDiff  \\\n",
       "0   32.7    16.0       105.5       ...              0.0            9.675181   \n",
       "1   33.5     2.0       105.0       ...            -13.0           17.125276   \n",
       "2   33.5     2.0        92.2       ...             -5.0            3.837585   \n",
       "3   29.5     3.0        92.2       ...             -4.0            0.753667   \n",
       "4   29.0     1.0       101.8       ...            -15.0           17.271949   \n",
       "\n",
       "   TrankTempoDiff  KenpomTempoAbsDiff  xOffyDefTrankAvg  yOffxDefTrankAvg  \\\n",
       "0             3.7              2.9896             101.4             94.65   \n",
       "1             2.3              2.8618             111.1             97.15   \n",
       "2             7.9              7.8767             104.7            100.95   \n",
       "3             0.0              0.1964             103.7            102.70   \n",
       "4             1.3              1.0755             111.4             95.45   \n",
       "\n",
       "   yOffxDefTODiff  xOffyDefTOAvg  xOffyDefRebDiff  yOffxDefRebDiff  \n",
       "0             0.4          22.85             -8.2             -1.0  \n",
       "1            -4.5          20.30              3.9              0.3  \n",
       "2            -8.6          20.75              2.9              0.6  \n",
       "3            -3.7          21.80              5.1              4.6  \n",
       "4            -2.2          20.10              6.7              5.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_cols = []\n",
    "for i in range(len(log1.coef_[0])):\n",
    "    if log1.coef_[0][i] == 0.0:\n",
    "        zero_cols.append(X.columns[i])\n",
    "nonzero_X = X.drop(columns = zero_cols)\n",
    "nonzero_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset above contains the features we will use in KNN. Notice that it is now only 21 features rather than 100+ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new training and testing sets, using the same random_state and test size so we have the same test set as before\n",
    "nX_train, nX_test, y_train, y_test = train_test_split(nonzero_X, y, test_size = 0.1, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use KNN, you must scale your features to the same scale so each feature has the same influence on similarity so we'll scale the data below. When scaling, you must only scale based off of data in the training set and then apply the same scale to the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.645269</td>\n",
       "      <td>0.586777</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.451327</td>\n",
       "      <td>0.020270</td>\n",
       "      <td>0.475336</td>\n",
       "      <td>0.240223</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.383871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.207332</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.176162</td>\n",
       "      <td>0.338346</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.432039</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.619497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.828930</td>\n",
       "      <td>0.561983</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.761062</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>0.838565</td>\n",
       "      <td>0.391061</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.212903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.014480</td>\n",
       "      <td>0.526515</td>\n",
       "      <td>0.160495</td>\n",
       "      <td>0.182957</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.660194</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.544025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.312378</td>\n",
       "      <td>0.798898</td>\n",
       "      <td>0.309028</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.584071</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923767</td>\n",
       "      <td>0.407821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.493548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687881</td>\n",
       "      <td>0.715909</td>\n",
       "      <td>0.472887</td>\n",
       "      <td>0.616541</td>\n",
       "      <td>0.303719</td>\n",
       "      <td>0.641860</td>\n",
       "      <td>0.597087</td>\n",
       "      <td>0.755639</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.746649</td>\n",
       "      <td>0.608815</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.424779</td>\n",
       "      <td>0.641892</td>\n",
       "      <td>0.717489</td>\n",
       "      <td>0.441341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.240405</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.132653</td>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.324380</td>\n",
       "      <td>0.595349</td>\n",
       "      <td>0.334951</td>\n",
       "      <td>0.586466</td>\n",
       "      <td>0.641509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.709344</td>\n",
       "      <td>0.705234</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.203540</td>\n",
       "      <td>0.445946</td>\n",
       "      <td>0.278027</td>\n",
       "      <td>0.513966</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.283871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.005637</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.577216</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.281553</td>\n",
       "      <td>0.349624</td>\n",
       "      <td>0.588050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.645269  0.586777  0.343750  0.223214  0.451327  0.020270  0.475336   \n",
       "1  0.828930  0.561983  0.305556  0.446429  0.761062  0.378378  0.838565   \n",
       "2  0.312378  0.798898  0.309028  0.580357  0.584071  0.500000  0.923767   \n",
       "3  0.746649  0.608815  0.031250  0.089286  0.424779  0.641892  0.717489   \n",
       "4  0.709344  0.705234  0.652778  0.812500  0.203540  0.445946  0.278027   \n",
       "\n",
       "         7         8         9     ...           11        12        13  \\\n",
       "0  0.240223  0.200000  0.383871    ...     0.272727  0.207332  0.333333   \n",
       "1  0.391061  0.266667  0.212903    ...     0.681818  0.014480  0.526515   \n",
       "2  0.407821  0.000000  0.493548    ...     0.000000  0.687881  0.715909   \n",
       "3  0.441341  0.000000  0.216129    ...     0.363636  0.240405  0.511364   \n",
       "4  0.513966  0.666667  0.283871    ...     0.681818  0.005637  0.750000   \n",
       "\n",
       "         14        15        16        17        18        19        20  \n",
       "0  0.176162  0.338346  0.528926  0.860465  0.432039  0.605263  0.619497  \n",
       "1  0.160495  0.182957  0.545455  0.651163  0.660194  0.894737  0.544025  \n",
       "2  0.472887  0.616541  0.303719  0.641860  0.597087  0.755639  0.500000  \n",
       "3  0.132653  0.228070  0.324380  0.595349  0.334951  0.586466  0.641509  \n",
       "4  0.577216  0.368421  0.685950  0.600000  0.281553  0.349624  0.588050  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale = MinMaxScaler()\n",
    "nX_train = pd.DataFrame(scale.fit_transform(nX_train))\n",
    "nX_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.723534</td>\n",
       "      <td>0.468320</td>\n",
       "      <td>0.302083</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.336283</td>\n",
       "      <td>0.263514</td>\n",
       "      <td>0.372197</td>\n",
       "      <td>0.480447</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.396774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.070496</td>\n",
       "      <td>0.628788</td>\n",
       "      <td>0.363253</td>\n",
       "      <td>0.240602</td>\n",
       "      <td>0.564050</td>\n",
       "      <td>0.562791</td>\n",
       "      <td>0.383495</td>\n",
       "      <td>0.609023</td>\n",
       "      <td>0.462264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.762746</td>\n",
       "      <td>0.520661</td>\n",
       "      <td>0.434028</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.407080</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.529148</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.625806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.367424</td>\n",
       "      <td>0.117439</td>\n",
       "      <td>0.466165</td>\n",
       "      <td>0.816116</td>\n",
       "      <td>0.474419</td>\n",
       "      <td>0.160194</td>\n",
       "      <td>0.477444</td>\n",
       "      <td>0.600629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.984225</td>\n",
       "      <td>0.694215</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.566372</td>\n",
       "      <td>0.601351</td>\n",
       "      <td>0.780269</td>\n",
       "      <td>0.251397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.320396</td>\n",
       "      <td>0.137845</td>\n",
       "      <td>0.533058</td>\n",
       "      <td>0.404651</td>\n",
       "      <td>0.660194</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.830189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.689595</td>\n",
       "      <td>0.421488</td>\n",
       "      <td>0.170139</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.623318</td>\n",
       "      <td>0.648045</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>0.553030</td>\n",
       "      <td>0.257026</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.456612</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.689320</td>\n",
       "      <td>0.620301</td>\n",
       "      <td>0.559748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.790595</td>\n",
       "      <td>0.793388</td>\n",
       "      <td>0.184028</td>\n",
       "      <td>0.133929</td>\n",
       "      <td>0.539823</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.735426</td>\n",
       "      <td>0.575419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.265204</td>\n",
       "      <td>0.738636</td>\n",
       "      <td>0.571529</td>\n",
       "      <td>0.365915</td>\n",
       "      <td>0.427686</td>\n",
       "      <td>0.544186</td>\n",
       "      <td>0.514563</td>\n",
       "      <td>0.853383</td>\n",
       "      <td>0.424528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.723534  0.468320  0.302083  0.303571  0.336283  0.263514  0.372197   \n",
       "1  0.762746  0.520661  0.434028  0.625000  0.407080  0.337838  0.529148   \n",
       "2  0.984225  0.694215  0.138889  0.580357  0.566372  0.601351  0.780269   \n",
       "3  0.689595  0.421488  0.170139  0.196429  1.000000  0.567568  0.623318   \n",
       "4  0.790595  0.793388  0.184028  0.133929  0.539823  0.432432  0.735426   \n",
       "\n",
       "         7         8         9     ...           11        12        13  \\\n",
       "0  0.480447  0.266667  0.396774    ...     0.363636  0.070496  0.628788   \n",
       "1  0.379888  0.400000  0.625806    ...     0.727273  0.000232  0.367424   \n",
       "2  0.251397  0.000000  0.000000    ...     0.681818  0.001486  0.261364   \n",
       "3  0.648045  0.133333  0.387097    ...     0.181818  0.152300  0.553030   \n",
       "4  0.575419  0.000000  0.177419    ...     0.318182  0.265204  0.738636   \n",
       "\n",
       "         14        15        16        17        18        19        20  \n",
       "0  0.363253  0.240602  0.564050  0.562791  0.383495  0.609023  0.462264  \n",
       "1  0.117439  0.466165  0.816116  0.474419  0.160194  0.477444  0.600629  \n",
       "2  0.320396  0.137845  0.533058  0.404651  0.660194  0.763158  0.830189  \n",
       "3  0.257026  0.190476  0.456612  0.627907  0.689320  0.620301  0.559748  \n",
       "4  0.571529  0.365915  0.427686  0.544186  0.514563  0.853383  0.424528  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nX_test = pd.DataFrame(scale.transform(nX_test))\n",
    "nX_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for KNN\n",
    "knn_params = {\n",
    "    'n_neighbors': [int(x) for x in np.linspace(1, 200, 201)],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, knn_params, cv = 9, scoring = 'neg_log_loss').fit(nX_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=40, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best hyperparamaters found in the grid search to minimize average log loss\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.566319894143969"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best average log loss achieved through the grid search\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.600555493230301"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The log loss of the KNN model on the unseen testing set\n",
    "predict = grid_search.predict_proba(nX_test)\n",
    "log_loss(y_test, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the Random Forest, L1, and L2 logistic regression models had the best performance for CV log loss, but only L1 and L2 logistic regression performed the best on the testing set. Therefore, when making predictions we will only use the L1 and L2 logitic regression models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
